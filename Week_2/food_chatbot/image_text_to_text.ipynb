{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee56dfd",
   "metadata": {},
   "source": [
    "Essential imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67fe0274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For image captioning\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# For embedding + vector DB\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# For chat\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90a815",
   "metadata": {},
   "source": [
    "Auto-Generate image caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "337aaa66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipForConditionalGeneration(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_decoder): BlipTextLMHeadModel(\n",
       "    (bert): BlipTextModel(\n",
       "      (embeddings): BlipTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): BlipTextEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BlipTextLayer(\n",
       "            (attention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BlipTextIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BlipTextOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BlipTextOnlyMLMHead(\n",
       "      (predictions): BlipTextLMPredictionHead(\n",
       "        (transform): BlipTextPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BLIP model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb674ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Image Caption: a bowl of tofu and some chops on a table\n"
     ]
    }
   ],
   "source": [
    "# Load and process image\n",
    "image = Image.open(\"../data/tofu.jpg\").convert(\"RGB\")\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "out = model.generate(**inputs)\n",
    "caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"üñºÔ∏è Image Caption:\", caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ebe661",
   "metadata": {},
   "source": [
    "Embed and Query ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f59d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed caption\n",
    "clip_model = SentenceTransformer(\"clip-ViT-B-32\")\n",
    "client = chromadb.PersistentClient(path=\"../chroma_db\")\n",
    "collection = client.get_collection(\"food_macros_clip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30436d",
   "metadata": {},
   "source": [
    "Chat agent with llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcf6b8f",
   "metadata": {},
   "source": [
    "Better prompt to get specific answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0666aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are a nutrition expert. Answer strictly based on the given food database context.\n",
    "\n",
    "Image Caption:\n",
    "{caption}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "If the answer is not in the context, say \"I don't know based on the given data.\"\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b5ecc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama2\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98b04667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ I can certainly help you with that! Based on the provided image and context, I would identify the food in the image as tofu. The bowl of tofu on the table suggests that it is a dish or preparation method involving tofu. Additionally, the presence of chops on the table could suggest that the dish may include meat as well, but the primary focus is on the tofu. Therefore, my answer would be:\n",
      "\n",
      "The food in the image is tofu.\n",
      "ü§ñ Sure! I'm happy to help. The foods in the image are:\n",
      "\n",
      "* Tofu\n",
      "* Chops (likely beef or pork)\n",
      "ü§ñ Sure! I'd be happy to help. The protein content of tofu can vary depending on the type and brand, but according to the USDA FoodData Central database, here are the approximate protein contents for different types of tofu:\n",
      "\n",
      "* Extra-firm tofu: 20 grams of protein per 3-ounce serving (about 1/4 cup)\n",
      "* Firm tofu: 15 grams of protein per 3-ounce serving (about 1/4 cup)\n",
      "* Soft or silken tofu: 7 grams of protein per 3-ounce serving (about 1/4 cup)\n",
      "\n",
      "So, based on the data provided, the approximate protein content of the tofu in the image is:\n",
      "\n",
      "* Extra-firm tofu: 20 grams\n",
      "* Firm tofu: 15 grams\n",
      "* Soft or silken tofu: 7 grams\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "ü§ñ Thank you for providing me with the context! Based on the food database provided, there are several plant-based protein sources that are similar to tofu in terms of nutritional profile and texture. Here are some options:\n",
      "\n",
      "1. Tempeh - Tempeh is a fermented soybean product that is high in protein, fiber, and vitamins. Like tofu, it has a nutty flavor and can be used in a variety of dishes.\n",
      "2. Seitan - Seitan is a meat substitute made from wheat gluten and has a chewy, meaty texture similar to tofu. It is high in protein and can be used in stir-fries, stews, and other dishes.\n",
      "3. Jackfruit - Jackfruit is a tropical fruit that can be used as a meat substitute due to its texture and nutritional profile. It is high in protein, fiber, and vitamins, and has a mild flavor that can be easily adapted to different dishes.\n",
      "4. Tofurky - Tofurky is a plant-based meat alternative made from soy protein and grains. It has a similar texture to tofu and can be used in a variety of dishes, including sausages and burgers.\n",
      "5. Veggie burgers - While not as high in protein as some of the other options, veggie burgers are made from plant-based protein sources such as beans, grains, and vegetables. They can be a good alternative to tofu for those looking for a lower-protein option.\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "ü§ñ \n",
      "Sure! Based on the image provided, it appears that the food in the bowl is tofu. If you are looking for a food similar to tofu, there are several options that are similar in terms of nutritional value and culinary uses. Here are some examples:\n",
      "\n",
      "1. Tempeh: Tempeh is made from fermented soybeans and has a nutty, earthy flavor. It is high in protein, fiber, and probiotics, making it a great alternative to tofu.\n",
      "2. Seitan: Seitan is a meat substitute made from wheat gluten and has a chewy, meat-like texture. It is high in protein and can be used in a variety of dishes, similar to tofu.\n",
      "3. Vegetarian chorizo: Chorizo is a spicy sausage that is typically made with pork. However, there are also vegetarian versions made with tofu or tempeh that have a similar flavor profile and texture.\n",
      "4. Tofurky: Tofurky is a meat substitute made from soy protein and can be used in place of chicken or turkey in many dishes. It has a similar texture and taste to tofu.\n",
      "5. Veggie burgers: Many veggie burgers are made with a variety of ingredients, including beans, grains, and vegetables. They can be a good substitute for tofu in sandwiches or wraps.\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "ü§ñ Ah, an excellent question! Based on the food database provided, there are several options that are similar to tofu in terms of nutritional content and health benefits. Here are some alternatives to tofu that you might find interesting:\n",
      "\n",
      "1. Tempeh - Like tofu, tempeh is made from soybeans but has a more nutty, earthy flavor. It's also higher in protein than tofu and has a similar texture.\n",
      "2. Seitan - Seitan is a meat substitute made from wheat gluten and has a chewy, meaty texture similar to tofu. It's also high in protein and can be used in a variety of dishes.\n",
      "3. Vegetarian chicken strips - If you're looking for something that tastes like chicken but is made from plant-based ingredients, vegetarian chicken strips could be a good option. They often contain soy protein and can be used in place of tofu in many recipes.\n",
      "4. Tofurky - Tofurky is a meat substitute made from soy protein and grains, similar to tofu. It has a meaty texture and can be used in place of chicken or beef in many dishes.\n",
      "5. Jackfruit - Jackfruit is a tropical fruit that can be used as a meat substitute due to its texture and versatility. It's high in protein and has a mild, slightly sweet flavor.\n",
      "\n",
      "I hope these suggestions are helpful! If you have any other questions or need more information, feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question = input(\"üß† Ask about the image/food: \")\n",
    "    if question.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    # Embed combined query for better precision\n",
    "    combined_query = f\"{caption}. Question: {question}\"\n",
    "    query_embedding = clip_model.encode(combined_query)\n",
    "\n",
    "    # Query ChromaDB\n",
    "    results = collection.query(query_embeddings=[query_embedding.tolist()], n_results=5, include=[\"documents\", \"distances\"])\n",
    "\n",
    "    # Filter precise matches (optional threshold tweakable)\n",
    "    filtered_docs = [\n",
    "        doc for doc, dist in zip(results[\"documents\"], results[\"distances\"])\n",
    "        if dist[0] < 0.2\n",
    "    ]\n",
    "    context_text = \"\\n\".join([d[0] for d in filtered_docs])\n",
    "\n",
    "    # Ask the model\n",
    "    response = chain.run(caption=caption, context=context_text, query=question)\n",
    "    print(\"ü§ñ\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2188c016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
